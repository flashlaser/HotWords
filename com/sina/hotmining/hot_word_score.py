# coding=utf-8

from sklearn.feature_extraction.text import CountVectorizer
from sklearn.feature_extraction.text import TfidfTransformer
import sys
import jieba
import jieba.analyse
import re

reload(sys)
sys.setdefaultencoding('utf-8')


## å¤„ç†ç‰¹æ®Šç¬¦å·
def filter_symbol(context):
    # httpæ­£åˆ™è¡¨è¾¾å¼è§„åˆ™
    re_http = re.compile(r'[a-zA-z]+://[^\s]*')
    # ä¸­è‹±æ–‡æ ‡ç‚¹ç¬¦å·æ­£åˆ™è¡¨è¾¾å¼
    re_punc = re.compile('[\s+\.\!\/_,$%^*(+\"\']+|[+â€”â€”ï¼ï¼Œã€‚ï¼Ÿã€~@#ï¿¥%â€¦â€¦&*ğŸ™„â€œâ€ã€Šã€‹ã€ã€‘ï¼šï¼ˆï¼‰]+'.decode('utf8'))

    context = context.decode('utf-8')
    context = context.strip().strip('\n')
    context = re_http.sub('', context)
    context = re_punc.sub('', context)

    return context


## è·å–å¾…å¤„ç†æ–‡æœ¬
def merge_sentence(context_path, col):
    sentence = ''
    for line in open(context_path, 'r').readlines():
        line = line.strip().strip('\n')
        items = line.split('\t')
        blog_content = items[col]
        context = filter_symbol(blog_content)
        sentence = ' '.join((sentence, context))  # ä¸€å¥ä¸€å¥çš„è¿æ¥
    return sentence


### æå–å¾®åšä¸­çš„çƒ­è¯
## ä½¿ç”¨jiebaä¸­çš„tf-idfè®¡ç®—çƒ­è¯åˆ†æ•°
def jieba_hot_word_score_tf_idf(sentence, top_k, with_weight, allow_POS=()):
    results_tf_idf = jieba.analyse.extract_tags(sentence, topK=top_k, withWeight=with_weight, allowPOS=allow_POS)
    return results_tf_idf


## ä½¿ç”¨jiebaä¸­çš„TextRankè®¡ç®—çƒ­è¯åˆ†æ•°
def jieba_hot_word_score_text_rank(sentence, top_k, with_weight, allow_POS=()):
    results_text_rank = jieba.analyse.textrank(sentence, topK=top_k)
    return results_text_rank


## ä½¿ç”¨jiebaä¸­çš„tf-idfæå–æ–‡æœ¬ä¸­å…³é”®å­—
def jieba_hot_word_tf_idf_text(in_path, out_path):
    out_file=open(out_path, 'w')
    for line in open(in_path, 'r').readlines():
        line=line.strip().strip()
        items=line.split('\t')
        blog_mid=items[0]
        blog_content=items[2]
        # è¿‡æ»¤ç‰¹æ®Šç¬¦å·
        sentence=filter_symbol(blog_content)
        # æå–æ¯æ¡å¾®åšçš„å…³é”®è¯
        key_words=jieba.analyse.extract_tags(sentence, topK=20, withWeight=False)
        if key_words:
            words=' '.join(key_words)
            result=blog_mid+'\t'+words
            out_file.write(result+'\n')
        else:
            continue
    out_file.close()


## ä½¿ç”¨jiebaä¸­çš„TF-IDFæå–listä¸­çš„å…³é”®å­—
def jieba_hot_word_tf_idf(blog_content_list):
    seg_words_list=[]
    for blog_content in blog_content_list:
        blog_content=blog_content.strip().strip('\n')
        # è¿‡æ»¤ç‰¹æ®Šç¬¦å·
        sentence = filter_symbol(blog_content)
        # æå–æ¯æ¡å¾®åšçš„å…³é”®è¯
        key_words = jieba.analyse.extract_tags(sentence, topK=20, withWeight=False, allowPOS=('ns', 'n', 'vn', 'v'))
        if key_words:
            seg_words_list.append(key_words)
        else:
            continue
    return seg_words_list


## ä½¿ç”¨jiebaä¸­çš„TextRankæå–æ–‡æœ¬ä¸­çš„å…³é”®å­—
def jieba_hot_word_text_rank_text(in_path, out_path):
    out_file = open(out_path, 'w')
    for line in open(in_path, 'r').readlines():
        line = line.strip().strip()
        items = line.split('\t')
        blog_mid = items[0]
        blog_content = items[2]
        # è¿‡æ»¤ç‰¹æ®Šç¬¦å·
        sentence = filter_symbol(blog_content)
        # æå–æ¯æ¡å¾®åšçš„å…³é”®è¯
        key_words = jieba.analyse.textrank(sentence, topK=20, withWeight=False)
        if key_words:
            words = ' '.join(key_words)
            result = blog_mid + '\t' + words
            out_file.write(result + '\n')
        else:
            continue
    out_file.close()


## ç»Ÿè®¡æ–‡æœ¬ä¸­çš„è¯é¢‘
def count_words_text(in_path, out_path):
    out_file=open(out_path, 'w')
    key_words={}
    for line in open(in_path, 'r').readlines():
        line=line.strip().strip('\n')
        items=line.split('\t')
        words=items[1]
        word_list=words.split(' ')
        # ç»Ÿè®¡è¯é¢‘
        for word in word_list:
            if word in key_words:
                key_words[word]=key_words[word]+1
            else:
                key_words[word]=1
    # å¯¹ç”Ÿæˆçš„è¯é¢‘æ’åº
    key_words_sort=sorted(key_words.items(), lambda x, y: cmp(x[1], y[1]), reverse=True)
    # å°†è¯é¢‘è¾“å…¥åˆ°æŒ‡å®šçš„æ–‡ä»¶ä¸­
    for item in key_words_sort:
        key=item[0]
        word=item[1]
        result=key+'\t'+str(word)
        out_file.write(result+'\n')
    out_file.close()


## ç»Ÿè®¡listä¸­çš„è¯é¢‘
def count_words(seg_words_list):
    key_words={}
    for seg_words in seg_words_list:
        for word in seg_words:
            if word in key_words:
                key_words[word]=key_words[word]+1
            else:
                key_words[word]=1
    # å¯¹ç”Ÿæˆçš„è¯é¢‘æ’åº
    key_words_sort = sorted(key_words.items(), lambda x, y: cmp(x[1], y[1]), reverse=True)
    key_words_list=[]
    for item in key_words_sort:
        word=item[0]
        word_count=[1]
        key_words_list.append(word)

    return key_words_list


## å–å‰kä¸ªå…³é”®è¯
def keywords_top(key_words_list, top_k):
    return key_words_list[0:top_k]


### æå–å¾®åšå„ä¸ªç±»åˆ«ä¸­çš„çƒ­è¯
## åˆ†è¯
def seg_word(sentence):
    seg_list = jieba.cut(str, cut_all=False)
    seg_str = ' '.join(seg_list)
    return seg_str


## ä½¿ç”¨sklearnä¸­çš„tf-idfæå–å…³é”®è¯
def sklearn_cal_hot_word_score_tf_idf(corpus):
    vectorizer = CountVectorizer()  # è¯¥ç±»ä¼šå°†æ–‡æœ¬ä¸­çš„è¯è¯­è½¬æ¢ä¸ºè¯é¢‘çŸ©é˜µï¼ŒçŸ©é˜µå…ƒç´ a[i][j] è¡¨ç¤ºjè¯åœ¨iç±»æ–‡æœ¬ä¸‹çš„è¯é¢‘
    transformer = TfidfTransformer()  # è¯¥ç±»ä¼šç»Ÿè®¡æ¯ä¸ªè¯è¯­çš„tf-idfæƒå€¼
    tfidf = transformer.fit_transform(
        vectorizer.fit_transform(corpus))  # ç¬¬ä¸€ä¸ªfit_transformæ˜¯è®¡ç®—tf-idfï¼Œç¬¬äºŒä¸ªfit_transformæ˜¯å°†æ–‡æœ¬è½¬ä¸ºè¯é¢‘çŸ©é˜µ
    word = vectorizer.get_feature_names()  # è·å–è¯è¢‹æ¨¡å‹ä¸­çš„æ‰€æœ‰è¯è¯­
    weight = tfidf.toarray()  # å°†tf-idfçŸ©é˜µæŠ½å–å‡ºæ¥ï¼Œå…ƒç´ a[i][j]è¡¨ç¤ºjè¯åœ¨iç±»æ–‡æœ¬ä¸­çš„tf-idfæƒé‡
    for i in range(len(weight)):  # æ‰“å°æ¯ç±»æ–‡æœ¬çš„tf-idfè¯è¯­æƒé‡ï¼Œç¬¬ä¸€ä¸ªforéå†æ‰€æœ‰æ–‡æœ¬ï¼Œç¬¬äºŒä¸ªforä¾¿åˆ©æŸä¸€ç±»æ–‡æœ¬ä¸‹çš„è¯è¯­æƒé‡
        for j in range(len(word)):
            print word[j], weight[i][j]


if __name__ == '__main__':
    print('start-------')
    blog_path='/home/littlebei/program/python/pycharm/HotWords/data/hotming/result/20170516/20170516_17.blog_users'

    # print('tf-idf--------------')
    # out_path_tf_idf='/home/littlebei/program/python/pycharm/HotWords/data/hotming/result/20170516/20170516_17.blog_hot_word_tf_idf'
    # out_path__tf_idf_count_words='/home/littlebei/program/python/pycharm/HotWords/data/hotming/result/20170516/20170516_17.blog_hot_word_tf_idf_count'
    # jieba_hot_word_tf_idf(blog_path, out_path_tf_idf)
    # count_words(out_path_tf_idf, out_path__tf_idf_count_words)
    #
    # print('text rank--------------')
    # out_path_text_rank='/home/littlebei/program/python/pycharm/HotWords/data/hotming/result/20170516/20170516_17.blog_hot_word_text_rank'
    # out_path_text_rank_count_words = '/home/littlebei/program/python/pycharm/HotWords/data/hotming/result/20170516/20170516_17.blog_hot_word_text_rank_count'
    # jieba_hot_word_text_rank(blog_path, out_path_text_rank)
    # count_words(out_path_text_rank, out_path_text_rank_count_words)

    sentence='çº¿ç¨‹æ˜¯ç¨‹åºæ‰§è¡Œæ—¶çš„æœ€å°å•ä½ï¼Œå®ƒæ˜¯è¿›ç¨‹çš„ä¸€ä¸ªæ‰§è¡Œæµï¼Œ'
    key_words=jieba.analyse.textrank(sentence, topK=10)
    for word in key_words:
        print word

    print(type(key_words))